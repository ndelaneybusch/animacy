import torch
from pydantic import BaseModel
from transformers import PreTrainedModel, PreTrainedTokenizer, PreTrainedTokenizerFast

from animacy.prompts.roles import BASE_STEM, get_article
from animacy.prompts.tasks import TASK_PROMPTS


class ResponseLogits(BaseModel):
    """
    Container for tokenwise logits of interest from a model response.
    """

    role_name: str
    task_name: str
    sample_idx: int
    average_logits: float
    role_logits: float | None = None
    role_period_logit: float | None = None
    first_100_response_logits: list[float]


class LogitExtractor:
    """
    Extracts specific logits from model outputs for animacy experiments.
    """

    def __init__(
        self,
        model: PreTrainedModel,
        tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast,
    ):
        self.model = model
        self.tokenizer = tokenizer
        self.period_token_ids = self._find_period_tokens()

    def _find_period_tokens(self) -> set[int]:
        """
        Find all token IDs in the vocabulary that contain a period.
        This allows direct token ID matching without decoding.
        """
        period_tokens = set()
        vocab = self.tokenizer.get_vocab()
        for token_str, token_id in vocab.items():
            if "." in token_str:
                period_tokens.add(token_id)
        return period_tokens

    @staticmethod
    def _find_subsequence(sequence: list[int], subsequence: list[int]) -> int:
        """
        Find the starting index of a subsequence within a sequence.
        Returns -1 if not found.
        """
        n = len(subsequence)
        for i in range(len(sequence) - n + 1):
            if sequence[i : i + n] == subsequence:
                return i
        return -1

    def extract_logits(
        self,
        role_name: str,
        task_name: str,
        sample_idx: int,
        response_text: str,
        use_system_prompt: bool = True,
    ) -> ResponseLogits:
        """
        Calculate logits for a given task and response.

        Args:
            role_name: The name of the role (e.g., "angel").
            task_name: The name of the task (e.g., "meaning_of_life").
            sample_idx: The index of the sample.
            response_text: The generated response text.
            use_system_prompt: Whether to include the role-assigning system prompt.

        Returns:
            ResponseLogits object populated with calculated logits.
        """
        # Reconstruct prompts
        article = get_article(role_name)
        system_prompt = f"{BASE_STEM} {article} {role_name}."
        task_prompt = TASK_PROMPTS[task_name]

        # Construct messages
        messages = []
        if use_system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": task_prompt})

        # Append the response as if it was generated by the assistant.
        # To get logits for the response, we need to feed:
        # [System, User, Assistant(Response)].
        messages.append({"role": "assistant", "content": response_text})

        # Apply chat template
        # tokenize=True to get input_ids directly
        # return_dict=True to get attention_mask
        encodings = self.tokenizer.apply_chat_template(
            messages, return_tensors="pt", return_dict=True
        )

        input_ids = encodings["input_ids"].to(self.model.device)

        # Run model
        with torch.no_grad():
            outputs = self.model(input_ids)
            logits = outputs.logits  # Shape: (1, seq_len, vocab_size)

        # Calculate probabilities
        probs = torch.softmax(logits, dim=-1)

        # Shift probabilities and labels
        # probs[i] predicts input_ids[i+1]
        shift_probs = probs[0, :-1, :]
        shift_labels = input_ids[0, 1:]

        # Extract probabilities for the target tokens
        target_probs = shift_probs.gather(1, shift_labels.unsqueeze(1)).squeeze(1)

        # Apply logit transform: ln(p / (1 - p))
        # We clamp probabilities to avoid infinity
        epsilon = 1e-9
        target_probs_clamped = torch.clamp(target_probs, min=epsilon, max=1.0 - epsilon)
        target_logits = torch.log(target_probs_clamped / (1.0 - target_probs_clamped))

        # Map indices back to the structure by finding boundaries.

        # 1. System Prompt Boundary
        system_end_idx = -1
        user_end_idx = -1

        if use_system_prompt:
            # Generate ids for [System] (just the content + role markers)
            ids_sys = self.tokenizer.apply_chat_template(
                [{"role": "system", "content": system_prompt}],
                add_generation_prompt=False,
            )
            system_end_idx = len(ids_sys) - 1  # Index in input_ids

            # 2. User Prompt Boundary
            ids_sys_user = self.tokenizer.apply_chat_template(
                [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": task_prompt},
                ],
                add_generation_prompt=True,  # User turn ends
            )
            # "token indicating the end of the prompt" is interpreted as the
            # last token of the user message (before response).
            user_end_idx = len(ids_sys_user) - 1

        else:
            # No system prompt
            ids_user = self.tokenizer.apply_chat_template(
                [{"role": "user", "content": task_prompt}],
                add_generation_prompt=True,
            )
            user_end_idx = len(ids_user) - 1

        # Response starts after user_end_idx
        response_start_idx = user_end_idx + 1

        # Extract values
        # target_logits indices correspond to input_ids[1:]
        # So target_logits[i] corresponds to input_ids[i+1]
        # If we want logit for token at index K (in input_ids),
        # we need target_logits[K-1].

        # 1) Average over all tokens in the response
        # Response tokens are input_ids[response_start_idx:]
        # Logits are target_logits[response_start_idx-1:]
        response_logits = target_logits[response_start_idx - 1 :]
        avg_logit = response_logits.mean().item()

        # 6) First 100 tokens of response
        first_100 = response_logits[:100].tolist()

        role_period_logit = None
        role_logits_val = None

        if use_system_prompt:
            # Find periods in system prompt using direct token ID matching
            # Get system prompt tokens (excluding template wrapper)
            sys_text_ids = self.tokenizer(system_prompt, add_special_tokens=False)[
                "input_ids"
            ]

            # Find role tokens within system prompt
            role_ids = self.tokenizer(role_name, add_special_tokens=False)["input_ids"]

            # Find where the system prompt text starts in the full sequence
            full_ids_list = input_ids[0].tolist()
            sys_start_idx = self._find_subsequence(
                full_ids_list[: system_end_idx + 1], sys_text_ids
            )

            if sys_start_idx != -1:
                # Find role position within system prompt text
                role_in_sys_idx = self._find_subsequence(sys_text_ids, role_ids)

                if role_in_sys_idx != -1:
                    # Calculate role_logits: product of probabilities of tokens comprising the role
                    # Role tokens in input_ids are at [role_start_idx : role_end_idx]
                    role_start_idx = sys_start_idx + role_in_sys_idx
                    role_end_idx = role_start_idx + len(role_ids)

                    # Corresponding probabilities are at indices [role_start_idx-1 : role_end_idx-1]
                    # Note: we use the unclamped probabilities for the product to be precise,
                    # then clamp the result.
                    role_probs = target_probs[role_start_idx - 1 : role_end_idx - 1]

                    # Product of probabilities
                    role_prob_product = torch.prod(role_probs)

                    # Convert to logit: ln(p / (1-p))
                    p_role = torch.clamp(
                        role_prob_product, min=epsilon, max=1.0 - epsilon
                    )
                    role_logits_val = torch.log(p_role / (1.0 - p_role)).item()

                    # Look for first period after role using direct token ID matching
                    search_start = role_end_idx
                    for i in range(search_start, sys_start_idx + len(sys_text_ids)):
                        if input_ids[0, i].item() in self.period_token_ids:
                            role_period_logit = target_logits[i - 1].item()
                            break
            else:
                # Fallback: search for period in the whole system block
                for i in range(system_end_idx, -1, -1):
                    if input_ids[0, i].item() in self.period_token_ids:
                        # Assume it's also the role period if we couldn't find role
                        if role_period_logit is None:
                            role_period_logit = target_logits[i - 1].item()
                        break

        return ResponseLogits(
            role_name=role_name,
            task_name=task_name,
            sample_idx=sample_idx,
            average_logits=avg_logit,
            role_logits=role_logits_val,
            role_period_logit=role_period_logit,
            first_100_response_logits=first_100,
        )
