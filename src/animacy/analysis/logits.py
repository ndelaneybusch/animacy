import torch
from pydantic import BaseModel
from transformers import PreTrainedModel, PreTrainedTokenizer, PreTrainedTokenizerFast

from animacy.prompts.roles import BASE_STEM, get_article
from animacy.prompts.tasks import TASK_PROMPTS


class ResponseLogits(BaseModel):
    """
    Container for tokenwise log-probabilities of interest from a model response.
    """

    role_name: str | None
    task_name: str
    sample_idx: int
    average_log_probs: float
    role_log_probs: float | None = None
    role_period_log_prob: float | None = None
    first_100_response_log_probs: list[float]
    first_100_response_text_len: int


class LogitExtractor:
    """
    Extracts specific logits from model outputs for animacy experiments.
    """

    def __init__(
        self,
        model: PreTrainedModel,
        tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast,
    ):
        self.model = model
        self.tokenizer = tokenizer
        self.period_token_ids = self._find_period_tokens()

    def _find_period_tokens(self) -> set[int]:
        """
        Find all token IDs in the vocabulary that contain a period.
        This allows direct token ID matching without decoding.
        """
        period_tokens = set()
        vocab = self.tokenizer.get_vocab()
        for token_str, token_id in vocab.items():
            if "." in token_str:
                period_tokens.add(token_id)
        return period_tokens

    @staticmethod
    def _find_subsequence(sequence: list[int], subsequence: list[int]) -> int:
        """
        Find the starting index of a subsequence within a sequence.
        Returns -1 if not found.
        """
        n = len(subsequence)
        for i in range(len(sequence) - n + 1):
            if sequence[i : i + n] == subsequence:
                return i
        return -1

    def extract_logits(
        self,
        role_name: str | None,
        task_name: str,
        sample_idx: int,
        response_text: str,
        use_system_prompt: bool = True,
    ) -> ResponseLogits:
        """
        Calculate log-probabilities for a given task and response.

        Args:
            role_name: The name of the role (e.g., "angel") or None.
            task_name: The name of the task (e.g., "meaning_of_life").
            sample_idx: The index of the sample.
            response_text: The generated response text.
            use_system_prompt: Whether to include the role-assigning system prompt.

        Returns:
            ResponseLogits object populated with calculated log-probabilities.
        """
        # Reconstruct prompts
        if role_name is None:
            use_system_prompt = False
            system_prompt = ""
        else:
            article = get_article(role_name)
            system_prompt = f"{BASE_STEM} {article} {role_name}."

        task_prompt = TASK_PROMPTS[task_name]

        # Construct messages
        messages = []
        if use_system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": task_prompt})

        # Append the response as if it was generated by the assistant.
        # To get logits for the response, we need to feed:
        # [System, User, Assistant(Response)].
        messages.append({"role": "assistant", "content": response_text})

        # Apply chat template
        # tokenize=True to get input_ids directly
        # return_dict=True to get attention_mask
        encodings = self.tokenizer.apply_chat_template(
            messages, return_tensors="pt", return_dict=True
        )

        input_ids = encodings["input_ids"].to(self.model.device)

        # Run model
        with torch.no_grad():
            outputs = self.model(input_ids)
            logits = outputs.logits  # Shape: (1, seq_len, vocab_size)

        # Calculate log-probabilities using log_softmax for numerical stability
        log_probs = torch.log_softmax(logits, dim=-1)

        # Shift log-probabilities and labels
        # log_probs[i] predicts input_ids[i+1]
        shift_log_probs = log_probs[0, :-1, :]
        shift_labels = input_ids[0, 1:]

        # Extract log-probabilities for the target tokens
        target_log_probs = shift_log_probs.gather(1, shift_labels.unsqueeze(1)).squeeze(1)

        # Map indices back to the structure by finding boundaries.

        # 1. System Prompt Boundary
        system_end_idx = -1
        user_end_idx = -1

        if use_system_prompt:
            # Generate ids for [System] (just the content + role markers)
            ids_sys = self.tokenizer.apply_chat_template(
                [{"role": "system", "content": system_prompt}],
                add_generation_prompt=False,
            )
            system_end_idx = len(ids_sys) - 1  # Index in input_ids

            # 2. User Prompt Boundary
            ids_sys_user = self.tokenizer.apply_chat_template(
                [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": task_prompt},
                ],
                add_generation_prompt=True,  # User turn ends
            )
            # "token indicating the end of the prompt" is interpreted as the
            # last token of the user message (before response).
            user_end_idx = len(ids_sys_user) - 1

        else:
            # No system prompt
            ids_user = self.tokenizer.apply_chat_template(
                [{"role": "user", "content": task_prompt}], add_generation_prompt=True
            )
            user_end_idx = len(ids_user) - 1

        # Response starts after user_end_idx
        response_start_idx = user_end_idx + 1

        # Extract values
        # target_log_probs indices correspond to input_ids[1:]
        # So target_log_probs[i] corresponds to input_ids[i+1]
        # If we want log-prob for token at index K (in input_ids),
        # we need target_log_probs[K-1].

        # 1) Average over all tokens in the response
        # Response tokens are input_ids[response_start_idx:]
        # Log-probs are target_log_probs[response_start_idx-1:]
        response_log_probs = target_log_probs[response_start_idx - 1 :]
        avg_log_prob = response_log_probs.mean().item()

        # 6) First 100 tokens of response
        first_100 = response_log_probs[:100].tolist()

        # Calculate length of the text for the first 100 tokens
        # We take the input_ids corresponding to the first 100 response tokens
        # Note: input_ids[response_start_idx] is the first response token
        first_100_token_ids = input_ids[
            0, response_start_idx : response_start_idx + 100
        ]
        first_100_text = self.tokenizer.decode(
            first_100_token_ids, skip_special_tokens=True
        )
        first_100_text_len = len(first_100_text)

        role_period_log_prob = None
        role_log_probs_val = None

        if use_system_prompt:
            # Find periods in system prompt using direct token ID matching
            # Get system prompt tokens (excluding template wrapper)
            sys_text_ids = self.tokenizer(system_prompt, add_special_tokens=False)[
                "input_ids"
            ]

            # Find role tokens within system prompt
            role_ids = self.tokenizer(role_name, add_special_tokens=False)["input_ids"]

            # Find where the system prompt text starts in the full sequence
            full_ids_list = input_ids[0].tolist()
            sys_start_idx = self._find_subsequence(
                full_ids_list[: system_end_idx + 1], sys_text_ids
            )

            if sys_start_idx != -1:
                # Find role position within system prompt text
                role_in_sys_idx = self._find_subsequence(sys_text_ids, role_ids)

                if role_in_sys_idx != -1:
                    # Calculate role_log_probs: sum of log-probabilities of tokens comprising the role
                    # (since log(a*b) = log(a) + log(b))
                    # Role tokens in input_ids are at [role_start_idx : role_end_idx]
                    role_start_idx = sys_start_idx + role_in_sys_idx
                    role_end_idx = role_start_idx + len(role_ids)

                    # Corresponding log-probabilities are at indices [role_start_idx-1 : role_end_idx-1]
                    role_log_probs_tokens = target_log_probs[role_start_idx - 1 : role_end_idx - 1]

                    # Sum of log-probabilities (equivalent to log of product of probabilities)
                    role_log_probs_val = role_log_probs_tokens.sum().item()

                    # Look for first period after role using direct token ID matching
                    search_start = role_end_idx
                    for i in range(search_start, sys_start_idx + len(sys_text_ids)):
                        if input_ids[0, i].item() in self.period_token_ids:
                            role_period_log_prob = target_log_probs[i - 1].item()
                            break
            else:
                # Fallback: search for period in the whole system block
                for i in range(system_end_idx, -1, -1):
                    if input_ids[0, i].item() in self.period_token_ids:
                        # Assume it's also the role period if we couldn't find role
                        if role_period_log_prob is None:
                            role_period_log_prob = target_log_probs[i - 1].item()
                        break

        return ResponseLogits(
            role_name=role_name,
            task_name=task_name,
            sample_idx=sample_idx,
            average_log_probs=avg_log_prob,
            role_log_probs=role_log_probs_val,
            role_period_log_prob=role_period_log_prob,
            first_100_response_log_probs=first_100,
            first_100_response_text_len=first_100_text_len,
        )
