<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Animacy Project</title>
  <link rel="stylesheet" href="styles.css">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
</head>

<body class="landing-page">
  <div class="landing-container">
    <header class="landing-header">
      <div class="logo">Animacy</div>
      <nav>
        <a href="https://github.com/ndelaneybusch/animacy" target="_blank" class="nav-link">GitHub</a>
      </nav>
    </header>

    <main class="hero-section">
      <h1 class="hero-title">Exploring Role-Playing in LLMs</h1>
      <p class="hero-subtitle">
        Investigating how Large Language Models adopt and maintain personas through quantitative analysis and
        qualitative response inspection.
      </p>

      <div class="hero-actions">
        <a href="viewer.html" class="btn btn-primary">
          Open Response Viewer
          <span class="arrow">â†’</span>
        </a>
      </div>

      <div class="markdown-body">
        <h1>Experiment Overview</h1>
        <p><img alt="experiment overview" src="images/image-7.png" /></p>
        <p>Without the role-providing system prompt, the initially find the role-playing responses deeply surprising,
          but then is gradually able to infer the role and approximately predict consistent behavior.</p>
        <p><img
            alt="gemma-3-27b-it begins its turn as the assistant, rapidly infers the role, but never fully reaches the token probabilities induced by the system prompt"
            src="images/image-4.png" /></p>
        <h1>Steering with Role Vectors restored most of the expected behavior.</h1>
        <p><img alt="steering restored response probabilities" src="images/image-9.png" /></p>
        <p>Surprisingly, high-animacy roles like lawyer, dragon, and biologist had the weakest steerability, across all
          six different methods we tried for computing Role Vectors.</p>
        <p><img alt="steerability by animacy groups" src="images/image-24.png" /></p>
        <p>We replicated this finding for new tasks (not used to derive the role vectors) designed to evaluate word
          availability.</p>
        <p><img alt="Word guess task" src="images/image-11.png" /></p>
        <p><img alt="Steerability by animacy groups on word guess task"
            src="images/word_guess_steerability_by_animacy_groups.png" /></p>\
        <h1>What makes the high mental animacy roles less steerable?</h1>
        <p>We found that mental animacy was more important than physical animacy in driving steerability. High mental
          animacy made the role vectors <em>less</em> steerable.</p>
        <p><img alt="Steerability by mental and physical animacy ratings" src="images/image-25.png" /></p>
        <p>This was particularly true for goal-directed behavior.</p>
        <p><img alt="Steerability by Goals" src="images/image-26.png" /></p>
        <h1>Discussion</h1>
        <p>Roles are heuristics for coordinating behavior across complex action spaces. Surprisingly, we find that roles
          with goals and high mental animacy tended to have weaker steering vectors across several different methods
          for deriving role vectors. Behaviorally, we also found that assistant-like roles like lawyer, professor etc.
          tended to give responses under roleplay prompting that were least different from responses given without role
          prompting, occassionally even half-triggering the instruction assistant training and denying that lawyers and
          biologists have internal experiences.</p>
        <p>It's not clear why socks and elm trees should involve more potent heuristics/latents than, say, a lawyer or
          a biologist when discussing inner thoughts, dreams, hopes for the future, things they're afraid of etc. Socks
          and elm trees don't do those things. We were not able to account for our results with word frequency.
          One possibility is that the models have a broader, more diffuse basin for the internal experience of people:
          they were trained on many more examples of physicians with diverse motivations, behaviors, and internal
          experiences. There might be a few examples of the anthropomorphized internal experiences of socks and elm
          trees, but not many. It's possible the role-play vectors are cleaner for roles that are less nuanced or
          complex.</p>
        <p>I suspect that this is a way that llm cognition is distinct from human cognition. Humans have an animacy
          bias that makes it easier to attribute internal experiences to animate objects - we evolved to model the
          mental states of living things. It's possible that llms are not so restricted. They were trained on human
          texts that may reflect these biases, but the scattered anthropomorphized accounts of inanimate things may
          be high-quality and not so diffuse or conflicting. Or perhaps the simulation machinery is less strongly
          bounded when it is guided by next-token prediction instead of embodied evolutionary pressures.</p>
        <p>Additional findings are available on the github README.</p>
      </div>
    </main>

    <footer class="landing-footer">
      <p>&copy; 2025 Animacy Project</p>
    </footer>
  </div>
</body>

</html>